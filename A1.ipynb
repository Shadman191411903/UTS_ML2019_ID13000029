{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Assignment 1-GAN-130000029</h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://github.com/Shadman191411903/UTS_ML2019_ID13000029/blob/master/A1.ipynb\">Github Link</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>1. Introduction</h2>\n",
    "<hr/>\n",
    "<div style=\"line-height: 1.8; font-size: 20px;font-family: Arial, Helvetica, sans-serif\">\n",
    "<p>\n",
    "\n",
    "The paper I found the most interesting is that of the General adversarial nets (GAN) by Ian GoodFellow. My first introduction to GAN was when I took the course on deep learning and convolutional neural networks. However, we briefly touched that topic and I did not understand this generative and discriminatory models that GAN proposed.  By reading and analysing this paper, I got to understand the intuition, theory and what problem GAN attempts to solve.\n",
    "\n",
    "</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>2. Content</h2>\n",
    "<hr/>\n",
    "<div style=\"line-height: 1.8; font-size: 18px;font-family: Arial, Helvetica, sans-serif\">\n",
    "<p>\n",
    "\n",
    "GAN attempts to solve the task of generative modelling. To perform generative modelling either a large collections of examples must be inputted to the model to find the probability density function that describes those examples, or learn a function that can generate more samples from the same training distribution. \n",
    "\n",
    "The authors provides two models within the GAN framework to solve the generative modelling tasks, both which ideally should be neural networks. The first neural network in this GAN framework is the generative model (G) which generates data, the second one is the discriminatory model (D) that examines this input and estimates whether the data generated by neural net G is authentic or not. The goal of model G is to trick the model D and eventually over time the model G has to create data that is as close as possible to the data distribution in the training set. In the GAN framework both the generator and discriminatory network simultaneously compete with each other.\n",
    "\n",
    "<br>\n",
    "\n",
    "<b><u>2.1 Training the GAN model.</u></b>\n",
    "\n",
    "<br>\n",
    "\n",
    "<ol>\n",
    "  <li>\tThe GAN model is trained by first feeding the generator network some randomly generated noisy inputs to generate some sample outputs. </li>\n",
    "  <li>Then some sample output from the real training set is taken and mixed with the generated output; which serves as an input to the discriminatory network.</li>\n",
    "  <li>The discriminatory network is then trained on this mixed set and tries to classify the input as real or fake. </li>\n",
    "    <li>As the discriminator slowly learns to classify the fake input, the discriminator is fed more inputs that more resembles data from the data distribution in the training set. This step is done to train the generator so that the feedback from the discriminator is used to update the weights of the generator. This is how the GAN model teaches the Generator to make outputs similar to the training data set and the Discriminator to get better at recognizing the actual training data then the sample/fake ones generated by the Generator.</li>\n",
    "    <li>The GAN model is trained in an adversarial way since the output from the generator network serves as the worst possible input on which the discriminatory model is trained</li>\n",
    "</ol> \n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<b><u>2.2 Mathematics behind the GAN. </u></b>\n",
    "\n",
    "<br>\n",
    "\n",
    "Let X be the true datataset, and Z be the normal distributed noise sampled from a prior distribution that allows the generator to output many different inputs instead of outputting one realistic input from the dataset.\n",
    "<br>\n",
    "\n",
    "\n",
    "<u>Discriminator</u>\n",
    "\n",
    "<br>\n",
    "\n",
    "An input is sampled from X and fed into a parametrized differential function ie a deep neural network. The goal of this function is to make D(x) as close as possible to 1 so that it signifies it came from a real example.\n",
    "<br/>\n",
    "\n",
    "<u>Generator</u>\n",
    "\n",
    "<br>\n",
    "\n",
    "After the input noise is sampled a differentiable generator function is applied to it. This output from the model is fed into the input of the Discriminator function which tries to make the output of D(G(z)) near 0.\n",
    "\n",
    "<br>\n",
    "\n",
    "Since D(x) represents the probability the data came from real dataset X, we train D to maximize the probability log(D(x)) and train G to minimize log(1-D(G(z)) i.e. train the discriminator to make D(G(z)) near 0 and train the generator to make D(G(z)) near 1.\n",
    "\n",
    "<br>\n",
    "\n",
    "In short these two neural networks play a game of minimax with each other and obtain global optimality.\n",
    "\n",
    "<hr/>\n",
    "\n",
    "<br>\n",
    "\n",
    "Experiments were carried out using the GAN framework on the MNIST database and using the MNIST database, the Toronto face database and CIFAR-10.  The experiments showed that the GAN model performed well with digits and faces but it created blurry and vague images on the CIFAR-10 dataset.\n",
    "\n",
    "<br>\n",
    "\n",
    "The advantages of using GAN are that, generating a sample requires only one pass through the model, rather than an unknown number of iterations of a Markov Chain. The main disadvantage is finding the optimality of the two neural networks; since gradient descent sometimes fail to do this. \n",
    "\n",
    "\n",
    "\n",
    "</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>3. Innovation</h2>\n",
    "<hr/>\n",
    "<div style=\"line-height: 1.8; font-size: 20px;font-family: Arial, Helvetica, sans-serif\">\n",
    "<p>\n",
    "\n",
    "I felt the paper proposed framework for GAN to solve the difficult task of generative modelling is very innovative particularly on three grounds. \n",
    "\n",
    "As mentioned above GAN uses adversarial training and two players who are competing against each other. I had never come across this term adversarial and after understanding the mathematics and theory behind it I can somewhat gather my thoughts and understand that, even with two models competing against one another can result in a win-win situation. The minimax or zero-sum game of GAN also make it different than the classical optimization problem that I had encountered previously.\n",
    "\n",
    "The other thing I found really innovative was that how GAN generates data as close as possible to the training set. The deep learning neural networks that I studied and learnt previously had to be taught what the input looks like in order to classify an output. For example if I am trying to find kangaroos in new images, I have to train the neural network with enough pictures of labelled kangaroos. This can be exhaustive and labelling images takes a lot of time. (Practical experience from deep learning and convolutional neural network course ! ).  GAN’s generator does the almost reverse of neural networks classification problem. As described above it does not take raw input and maps it to output, but instead it tries to generate the input data that would map to that output. This can lead to creation of new images and eventually be applied to many fields.\n",
    "\n",
    "Finally, as seen in supervised learning for example K-nearest neighbour, the mean squared error is used to tell the model what the output should look like. So during training if the model produced an output but the label showed that it predicted wrongly, the mean distance is calculated and over time it can cause the model to learn blurry images or inputs. However, in GAN the discriminator learns how inputs and outputs should be paired. When the generator produces an output, the discriminator can choose to take that input or reject it, thus enabling many right answers and hence probability of the model learning blurry images is eliminated.\n",
    "\n",
    "Although the proposed GAN framework to me is very innovative there are still some shortcomings. For example, for GAN cannot simply create new images for nothing. For it to create new images it still needs a wealth of training data, that must be provided. For example in the MNIST dataset, it won’t work till we provided it some images of digits. If similar data is not-present the GAN model framework cannot be used.\n",
    "\n",
    "\n",
    "</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>4. Technical Quality</h2>\n",
    "<hr/>\n",
    "<div style=\"line-height: 1.8; font-size: 20px;font-family: Arial, Helvetica, sans-serif\">\n",
    "<p>\n",
    "\n",
    "I felt the paper was very technical to a person who has little to no background in computer science, statistics or mathematics. Even though I did my undergraduate in statistics and risk management I found particularly the mathematical equations and graphs difficult to understand.  Moreover, there is also no background on the what the discriminatory and the generative model in general does, hence further research into what discriminator and generative modelling had to be carried out to gain a better understanding of the underlying GAN’s architecture. Furthermore the background study as well as the comparison between how experimental result of GAN with other models is also too vague to understand and a few papers and journals had to be studied to comprehend the problem GAN was trying to solve. Finally, the results also overlooked the CIFAR-10 experiments performed using GAN, with related models. I felt it should have been important since the CIFAR-10 produced the worst result from the GAN experiments and it would have been good to know how the other models performed when GAN did the worst. Overall, I believe the paper was tailored for a particular audience and in general very technical.\n",
    "\n",
    "Given the general difficulty in reading this paper I believe the authors did a great job in giving an overview picture on how the two neural networks in GAN works. By saying that the generative model is like a team of counterfeiters trying to produce fake money without being detected and the Discriminative model is like the police, who always detect and stop the circulation of counterfeit money and overtime the competition by both sides to produce and detect the competition in this game drives both teams to improve their methods until the counterfeits are indistinguishable from the genuine articles really helped to visualize the goal GAN was trying to achieve.\n",
    "\n",
    "\n",
    "</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>5. Application and X-factor </h2>\n",
    "<hr/>\n",
    "<div style=\"line-height: 1.8; font-size: 20px;font-family: Arial, Helvetica, sans-serif\">\n",
    "<p>\n",
    "\n",
    "Since GAN is a generative model and it can create and modify new images, I believe there will a huge number of applications of the GAN framework. For example they have been used by Nvidia to correct images and reconstruct obscure parts (Stanislas Chaillou, 2019). More recently GAN’s are also used in autonomous driving vehicle. (Michael, et al., 2019)\n",
    "\n",
    "The various application of GAN can be further found here:\n",
    "https://machinelearningmastery.com/impressive-applications-of-generative-adversarial-networks/ \n",
    "\n",
    "With its huge number of applications I cannot also help think about ways in which GAN can also be used in a negative way. Since GAN can create new images, scammers can use GAN, to make exact copies of people’s identities, create fake content, news etc. Moreover, if artificial intelligence is used to detect cyber security threats, GAN can help find and identify weaknesses that may enable the hacker to penetrate the system.\n",
    "\n",
    "The authors had provided some extensions on this framework and new research on GAN’s has addressed this. The extensions the authors have proposed and the further developments carried out in research are listed below.\n",
    ".\n",
    "<br/>\n",
    "<ol>\n",
    "<li>‘ A conditional generative model p(x j c) can be obtained by adding c as input to both G and D.’\n",
    "-. Mirza, M. and Osindero, S. (2014) introduces conditional GAN’s where both the generator and discriminator are conditioned on some extra information y, which could be class labels or data or other forms</li>\n",
    "\n",
    "<li>‘ Semi-supervised learning: features from the discriminator or inference net could improve performance-‘\n",
    "Salimans, T., Goodfellow, I., Zaremba, W., Cheung, V., Radford, A. and Chen, X. (2016) in their paper tackles the problem of non-convergence during GAN training (i.e cost function of generator and discriminator cannot be simultaneously minimised). The results from their paper lead to improved performance in semi-supervised learning and sample generation of classifiers.</li>\n",
    "<li>‘ Efficiency improvements: training could be accelerated greatly by divising better methods for coordinating G and D or determining better distributions to sample z from during training’-\n",
    "\n",
    "\n",
    "\n",
    "Arjovsky, M. and Bottou, L. (2017) in their paper address and analyse these issues during GAN training. This paper explored the ways to tackle the problem of the generator getting worse as the discriminator gets better, unstable training and use of an alternative cost function.</li>\n",
    "\n",
    "</ol>\n",
    "<br/>\n",
    "Overall in my opinion this fundamental framework GAN paper has paved the way for a future of an exciting more generative modelling in artificial intelligence.\n",
    "\n",
    "</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>6. Presentation</h2>\n",
    "<hr/>\n",
    "<div style=\"line-height: 1.8; font-size: 20px;font-family: Arial, Helvetica, sans-serif\">\n",
    "<p>\n",
    "\n",
    "Overall the paper was very succinct and very much to the point. In my opinion the argument of how GAN works was really well presented as the it was described with an analogy.  However, I felt the felt short in most of the sections as the depth of the argument had little background information.  Since this is a new framework that is propsed on generative modelling I believe a lot of the technical terms should be described in a way that would be easy for any reader to understand. \n",
    "Nonetheless, when I understood the technical aspects and background information I found this paper very rich, informative and interesting. As a consequence negative issues described above and in the technical section were simply forgotten.\n",
    "\n",
    "\n",
    "\n",
    "</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>7. References</h2>\n",
    "<hr/>\n",
    "<div style=\"line-height: 1.8; font-size: 20px;font-family: Arial, Helvetica, sans-serif\">\n",
    "<p>\n",
    "<ol>\n",
    "\n",
    "<li> Michael, et al., 2019. Yes, we GAN: Applying Adversarial Techniques for Autonomous Driving. Ireland: arXiv:1902.03442.</li>\n",
    "\n",
    "<li> Arjovsky, M. and Bottou, L. (2017). TOWARDS PRINCIPLED METHODS FOR TRAINING GENERATIVE ADVERSARIAL NETWORKS. [online] Available at: https://arxiv.org/pdf/1701.04862.pdf [Accessed 25 Aug. 2019].</li>\n",
    "\n",
    "<li> https://machinelearningmastery.com/impressive-applications-of-generative-adversarial-networks/ (2019). 18 Impressive Applications of Generative Adversarial Networks (GANs). [online]Machine Learning Mastery. Available at: https://machinelearningmastery.com/impressive-applications-of-generative-adversarial-networks/ [Accessed 25 Aug. 2019].</li>\n",
    "\n",
    "<li> Mirza, M. and Osindero, S. (2014). Conditional Generative Adversarial Nets. [online] Available at: https://arxiv.org/pdf/1411.1784.pdf [Accessed 24 Aug. 2019].\n",
    "\n",
    "<li> Salimans, T., Goodfellow, I., Zaremba, W., Cheung, V., Radford, A. and Chen, X. (2016). Improved Techniques for Training GANs. [online] Available at: https://arxiv.org/pdf/1606.03498.pdf [Accessed 25 Aug. 2019].</li>\n",
    "\n",
    "<li> Stanislas Chaillou (2019). GAN Archives | NVIDIA Developer Blog. [online] NVIDIA Developer Blog. Available at: https://devblogs.nvidia.com/tag/gan/ [Accessed 27 Aug. 2019].</li>\n",
    "\n",
    "</ol>\n",
    "\n",
    "</p>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
